{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 20: GLM Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We continue with the model: $$Y = X\\beta+\\epsilon$$ with $\\epsilon\\tilde{}N(0,I\\sigma^2)$. The matrices X and Y are assumed to be known and the noise is assumed to be uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find the value of $\\beta$ that minimizes $$(Y-X\\beta)^T(Y-X\\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is the sums of squared errors (SSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case where we have one explanatory variable, we are looking for a line that minimizes the distance between all the data points and the line (in the y direction). With two explanatory variables we look for a plane, and with p explanatory variables we look for a p-dimensional hyperplane.\n",
    "<img src=\"onedim.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least squares criterion is:$$Q = (Y - X\\beta)'(Y-X\\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize this, we can take the derivative with repsect to $\\beta$ and set it equal to 0, which produces the normal equations:$$X'X\\hat{\\beta}=X'Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the ordinary least squares (OLS) estimators are given by:$$\\hat{\\beta} = (X'X)^{-1}X'Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Ordinary Least Squares Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS has a few interesting properties. First of all, the expected value of beta hat is equal to beta, so it is an <b>unbiased estimator</b>. Also any other unbiased estimator of beta will have a larger variance than the OLS solution. It is the Best Linear Unbiased Estimator (BLUE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that if $\\epsilon$ is independent and identically distributed (i.i.d.), then the OLS estimate is optimal.$$\\hat{\\beta} = (X'X)^{-1}X'Y$$\n",
    "\n",
    "However, if $Var(\\epsilon) = V\\sigma^2\\neq I\\sigma^2$ then the Generalized Least Squares (GLS) estimate is optimal.This means that we have to include the variance covariance matrix into our estimate of beta $$\\hat{\\beta} = (X'V^{-1}X)^{-1}X'V^{-1}Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note</b>: that the OLS solution is a special case of the GLS solution because if you put the Identity matrix into the GLS for V, it becomes the OLS solution. So when $\\epsilon$ is autocorrelated, we have to use GLS instead of OLS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we use our <b>model</b>$$Y = X\\beta + \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get our <b>estimate</b>$$\\hat{\\beta}=(X'V^{-1}X)^{-1}X'V^{-1}Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we solve and use to find our <b>fitted values</b>$$\\hat{Y}=X\\hat{\\beta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from which we can calculate our <b>residuals</b>:$$r=Y-\\hat{Y}$$\n",
    "$$ = (I-X(X'V^{-1}X)^{-1}X'V^{-1})Y$$\n",
    "$$ = RY$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we assume that $\\epsilon$ is i.i.d., we still need to estimate the residual variance, $\\sigma^2$. Our estimate is:$$\\hat{\\sigma}^2 = \\frac{r^Tr}{tr(RV)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is the transpose of the residuals times the residuals, over the trace of the residual inducing matrix (R) times V. Recall that the trace of an n x n matrix is the sum of the elements on the main diagonal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OLS:$$\\hat{\\sigma}^2 = \\frac{r^Tr}{N - p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where N is the length of the Y matrix and p is the number of columns in the design matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note</b>:Estimating $V\\neq I$ is more difficult and covered in the next module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Geometric Interpretation of the GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of y as a vector in a space with one dimension per observation. Imagine a study with 3 subjects and 2 predictor variables, X1 and X2. Y exists in a 3 dimensional space, and the model exists in a 2D space. All possible combinations of X1 and X2 span a 2D plane which is a subspace of the 3D space. The plane is the subspace spanned by the model: <img src='3dspace.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are we doing when we fit the linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are minimizing the sum of squared errors (Q). In solving for beta we are <b>projecting</b> the data (y) onto the subspace of X. The formula for $\\hat{\\beta}$ is actually projection matrix:\n",
    "<img src='projectionmatrix.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
